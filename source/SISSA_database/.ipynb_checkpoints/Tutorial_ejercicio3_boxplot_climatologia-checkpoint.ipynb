{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3fb5aa-3c1b-4bb7-96c4-7067593880fe",
   "metadata": {},
   "source": [
    "# Ejercicio 3 capacitación: climatologia de pronósticos GEFSv12 y ERA5\n",
    "\n",
    "En esta sección vamos a considerar un área de trabajo, que corresponde a SESA (Sudeste de Sudámerica) y a partir de un rango de días de pronóstico y un plazo de 10 años, vamos a ver como se comporta esa climatología con los valores obtenidos para un año en particular.\n",
    "\n",
    "Al igual que venimos haciendo en otros ejercicios, vamos a intentar trabajar sin descargar los datos y guardando en una variables que nos permita posteriormente procesar y graficar la figura que necesitamos.\n",
    "\n",
    "Vamos a comenzar primero detallando algunas variables para "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0690e56-bbe5-4d17-a5ec-a8bf15123ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta con datos ERA5: sissa-forecast-database/ERA5/tmax/\n",
      "Carpeta con datos GEFSv12 sin corregir: sissa-forecast-database/subseasonal/GEFSv12/tmax/\n",
      "Carpeta con datos GEFSv12 corregidos: sissa-forecast-database/subseasonal/GEFSv12/tmax/\n"
     ]
    }
   ],
   "source": [
    "# Como siempre, partimos importando los modulos a utilizar:\n",
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DATOS\n",
    "BUCKET_NAME = 'sissa-forecast-database'\n",
    "tforecast = 'subseasonal'\n",
    "m0 = 'ERA5'\n",
    "m1 = 'GEFSv12'\n",
    "m2 = 'GEFSv12_corr'\n",
    "variable = 'tmax'\n",
    "years = np.arange(2010, 2020)\n",
    "\n",
    "path0 = m0 + '/' + variable + '/' \n",
    "path1 = tforecast + '/' + m1 + '/' + variable + '/' \n",
    "path2 = tforecast + '/' + m2 + '/' + variable + '/' \n",
    "\n",
    "# Colocamos un área para SESA\n",
    "lat_n = -20.\n",
    "lat_s = -30.\n",
    "lon_w = -65.\n",
    "lon_e = -55.\n",
    "\n",
    "#Fecha de referencia:\n",
    "ref_date = dt.datetime(2010,12,1)\n",
    "\n",
    "# Umbrales a considerar para calcular probabilidad:\n",
    "umbrales = [1., 30., 50., 100.]\n",
    "\n",
    "print('Carpeta con datos ERA5:', BUCKET_NAME + '/' + path0)\n",
    "print('Carpeta con datos GEFSv12 sin corregir:', BUCKET_NAME + '/' + path1)\n",
    "print('Carpeta con datos GEFSv12 corregidos:', BUCKET_NAME + '/' + path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f12d92-70a6-433c-b828-d8deca212a38",
   "metadata": {},
   "source": [
    "**Descarga de datos subset**\n",
    "\n",
    "Los siguientes bloques corresponderan a las lineas de codigo para descargar los datos necesarios para revisar la climatologia del mes de diciembre\n",
    "y vamos a analizar como se ven las dos primeras semanas de pronostico.\n",
    "\n",
    "Creamos una carpeta temporal dentro de nuestra carpeta de trabajo.\n",
    "\n",
    "Partimos descargando ERA5\n",
    "\n",
    "Seguimos con GEFSv12\n",
    "\n",
    "y terminamos con GEFSv12_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f51f876-4c3e-4b15-a2fc-88a22ffedd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sissa-forecast-database/ERA5/tmax/2010.nc\n",
      "Descargando: ./tmp/era5_tmax_2010.nc\n",
      "s3://sissa-forecast-database/ERA5/tmax/2011.nc\n",
      "Descargando: ./tmp/era5_tmax_2011.nc\n",
      "s3://sissa-forecast-database/ERA5/tmax/2012.nc\n",
      "Descargando: ./tmp/era5_tmax_2012.nc\n",
      "s3://sissa-forecast-database/ERA5/tmax/2013.nc\n",
      "Descargando: ./tmp/era5_tmax_2013.nc\n",
      "s3://sissa-forecast-database/ERA5/tmax/2014.nc\n",
      "Descargando: ./tmp/era5_tmax_2014.nc\n",
      "s3://sissa-forecast-database/ERA5/tmax/2015.nc\n",
      "Descargando: ./tmp/era5_tmax_2015.nc\n",
      "s3://sissa-forecast-database/ERA5/tmax/2016.nc\n",
      "Descargando: ./tmp/era5_tmax_2016.nc\n",
      "s3://sissa-forecast-database/ERA5/tmax/2017.nc\n",
      "Descargando: ./tmp/era5_tmax_2017.nc\n",
      "s3://sissa-forecast-database/ERA5/tmax/2018.nc\n",
      "Descargando: ./tmp/era5_tmax_2018.nc\n",
      "s3://sissa-forecast-database/ERA5/tmax/2019.nc\n",
      "Descargando: ./tmp/era5_tmax_2019.nc\n"
     ]
    }
   ],
   "source": [
    "tmp = './tmp/'\n",
    "os.makedirs(tmp, exist_ok=True)\n",
    "\n",
    "for year in years:\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    awsfile = 's3://' + BUCKET_NAME + '/' + path0 + str(year) + '.nc'\n",
    "    print(awsfile)\n",
    "    with fs.open(awsfile) as f:\n",
    "        ncfile = tmp + 'era5_' + variable + '_' + str(year) + '.nc'\n",
    "        if not os.path.isfile(ncfile):\n",
    "            print('Descargando:', ncfile)\n",
    "            era = xr.open_dataset(f)\n",
    "            era = era.sel(latitude=slice(lat_n, lat_s), longitude=slice(lon_w, lon_e))\n",
    "            era.to_netcdf(ncfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9e407b-4df3-45d2-a551-fa772fece95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-12-01 00:00:00\n",
      "sissa-forecast-database/subseasonal/GEFSv12/tmax/2010/20101201/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ii' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m awsfiles \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mls(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m PATH) \n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ens, awsfile \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(awsfiles):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tmp/uncal_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m variable \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m loop_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mii\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(awsfile) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     14\u001b[0m             gefs \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ii' is not defined"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    this_date = ref_date.replace(year=year)\n",
    "    loop_date = this_date\n",
    "    while loop_date < (ref_date.replace(year=year) + dt.timedelta(days=30)):\n",
    "        print(loop_date)\n",
    "        print(loop_date.weekday())\n",
    "        if loop_date.weekday() == 2:\n",
    "            PATH = BUCKET_NAME + '/' + path1 + str(year) + '/' + loop_date.strftime('%Y%m%d') + '/'\n",
    "            print(PATH)\n",
    "            awsfiles = fs.ls('s3://' + PATH) \n",
    "            for ens, awsfile in enumerate(awsfiles):\n",
    "                if not os.path.isfile('./tmp/uncal_' + variable + '_' + loop_date.strftime('%Y%m%d') + '_' + str(ii) + '.nc'):\n",
    "                    with fs.open(awsfile) as f:\n",
    "                        gefs = xr.open_dataset(f)\n",
    "                        gefs = gefs.sel(lat=slice(lat_n, lat_s), lon=slice(lon_w, lon_e))\n",
    "                        gefs['M'] = ens\n",
    "                        ds = gefs[variable]\n",
    "                        ds.to_netcdf('./tmp/uncal_' + variable + '_' + loop_date.strftime('%Y%m%d') + '_' + str(ii) + '.nc')\n",
    "        loop_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f119b-0ca5-4611-9eee-f4a808e0f868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
